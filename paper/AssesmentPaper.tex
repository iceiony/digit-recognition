\documentclass[11]{article}

\usepackage[inline]{enumitem}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{float}

\usepackage[numbered,framed]{matlab-prettifier}
\let\ph\mlplaceholder % shorter macro
\lstMakeShortInline"

\lstset{
  style              = Matlab-editor,
  basicstyle         = \mlttfamily,
  escapechar         = ",
  mlshowsectionrules = true,
}

\title{
  Hand Written
  Digit Recognition
  }
  
\date{}
\author{Adrian Ionita\\
\small{(NR: 1057404, ID: AFI904)}}

\hyphenation{thatshouldnot}

\begin{document}
\maketitle 	

\section{Introduction}
This project designs and implements a neural network to optically recognise hand written digits. Starting with the fewest number of layers and increasing subsequently, the network generalisation is optimised through adjusting parameters of learning rate, batch size, epoch count, data preprocessing and weight decay rate.

The data set used contains records from 43 participants and is split between a training set and a test set. The training set of 3823 records was collected from 30 of the participants. The test set of 1797 records was collected from the remaining 13. Each record contains 64 pixels of an 8x8 image (e.g. \ref{fig:digits} followed by the digit it represents. The pixel intensity is between 0 and 16. 

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{zero.png}
\includegraphics[width=0.4\textwidth]{seven.png}
\caption{Example images from training set}
\label{fig:digits}
\end{figure}
\section{Design}

The resulting design is a two layer perceptron neural network using the cross entropy cost function. This cost function was selected as the problem is one of classification rather than regression (i.e. each feature set belongs to a digit class).

When using cross entropy, the activation function generally used is a sigmoid. As its output ranges from 0 to 1, the activation of a neuron is analogous to a probability function. Whilst sigmoid is used for hidden layer activations, softmax is used for the output layer. This ensures that the probability of each class activation does not exceed 1. 

The network's input is 69 features. The fist 64 represent the image pixels with an additional 5 features extracted from pixel covariance and mean location.  The hidden layer size is 60 nodes and the output layer is 10 nodes ( one for each activation class ). 

Additional training records were created by rotating each feature set to a varying range between -20 and +20 degrees clockwise (fig. \ref{fig:rotated}. This is in order to aid detection of digits that are not properly aligned. The additional covariance and mean features of each record served for the same purpose. 
\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{seven.png}
\includegraphics[width=0.4\textwidth]{seven_rotated.png}
\caption{Example images from training set}
\label{fig:rotated}
\end{figure}
\section{Design}

The network is trained off-line with a fixed batched size that was optimised experimentally. Batch training ensures a more smooth decent of the gradient error cost. Additionally a varying learning rate is used. This decreases in value with each complete iteration of the training set. The decay of the learning rate was optimised experimentally. There is one single learning rate used for both the output layer and hidden layers.

Network training stops after a fixed number of epochs was trained. An epoch was considered as one single update of the weights. Although the network may stop at a point where the error cost is high, the approach avoid situations where the network would never complete training ( i.e. when choosing an error cost that can't be achieved ). The optimal number of epochs was also discovered experimentally. 

Network training also employs a simple weight decay function ( i.e. decreasing the weights by a ratio of their current value. The decay ratio was optimised experimentally. 

\section{Implementation}

The neural network was implemented using matlab. Even though matlab has a specialised toolbox for neural networks, the project implements its own network and back-propagation algorithm. Matlab was chosen do to its in built matrix operations, mathematical tools and visualisation features (e.g. error cost plotting). When adding image rotations to the training set, the image processing toolbox further simplified code complexity as algorithms were readily available.

\subsection{Initialisation}

The entry file for training the network is \emph{train.m}. At the start of execution the training set is loaded and extended with additional records for digit rotations (using \emph{addrotations.m}). The resulting training set is three times larger, having one clockwise and one counter clockwise rotation for each digit. The image processing toolbox from matlab was used for this. Resulting records were arbitrarily shuffled.

The set was further extended with additional features (i.e. covariance and mean pixel location). After extension, the features are normalised and scaled up ( i.e. by multiplying each feature to a larger number). The feature extension and normalisation form a single step named preprocessing (handled by \emph{preprocess.m}).

The target class for the training set is separated from the rest of the features. Each target is transformed from a single digit to a 10 dimension vector. The vector represents the probability of each class occurring, meaning it has the value 1 on the class's position and 0 otherwise. 

\subsection{Training}

When training a network, it is first initialised with random weights drawn from a uniform distribution of $[-0.5,+0.5]$. The training is implented using vector and matrix multiplication instead of iterating through individual positions of weight updates. This takes advantage of matlab's optimisations in matrix operations and serves for faster training. 

The output of the hidden layer is first calculated using the sigmoid activation function on an input batch. It then gets used as input in calculating the total network output using the softmax function. These are handled, in vector notation, by \emph{sigmoid.m} and \emph{softmax.m}.
\begin{lstlisting}
        out{1} = sigmoid(in_batch,w{1});
        out{2} = softmax(out{1},w{2});
\end{lstlisting}

The delta of each weight update is calculated using the first derivative of the cost function for each layer (following course formulas). The weight are updated by the delta and also suffer a weight decay proportional to their current value. 

\subsection{Testing}

After training, the network weights are measured on the loaded test set. The set is processed similarly to the training one(i.e. through additional features and normalisation). When calculating the result of a given input, the predicted class is determined by selecting the output position with the highest probability value. Network performance is determined from the number of correctly predicted classes. 

\section{Experiments}

\subsection{Single Layer Experiments}
In order to establish a baseline of performance the network was initially built as a single layer perceptraon network with a fixed learning rate($\mu$), number of epochs and batch size. Each experiment trained and measured network performance 10 times for a better estimation of optimisation improvements. 

\subsubsection{Learning Rate}

For the initial training no preprocessing was done on the data set. Because of the softmax activation function, a batch size larger than 20 or a learning rate larger than 0.01 could not be used as the the function would overflow double precision data types.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{learning_rate.png}
\caption{Adjusting learning rate $\mu$}
\label{fig:learningrate}
\end{figure}

With the initial batch=20, epoch=2000 and $\mu=0.01$, the network achieved 93.25\%($\pm 1.4\%)$ prediction success on the testing set.  A lower learning rate of $\mu=0.001$ was tried which produced a performance of 92.59\%($\pm 0.3\%$). Although the average performance was lower the deviation in learning was greatly reduced. Using a variable learning rate the performance was further improved to 94.57\%($\pm 0.07\%$) signifying a great improvement in both performance and stability (fig. \ref{fig:learningrate}). The learning rate decay per each complete iteration of the training set is defined as $\mu = \mu / 1.1$ starting with $\mu=0.01$. Reducing the rate by larger or lower ratio would not result in lower performance.


\subsubsection{Batch and Epoch Sizes}
Whilst a varying learning rate increase performance significantly, batch and epoch sizes were also experimented with (fig. \ref{fig:batchandepoch}). A low epoch size would underfit general training performance, whilst a low batch size would lead to larger deviations (as the gradient direction would be less stable). No performance improvements were achieved at this point. 

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{batchandepoch.png}
\caption{Adjusting the batch and epoch sizes}
\label{fig:batchandepoch}
\end{figure}

\subsubsection{Preprocessing}
To further increase performance, preprocessing was done to both the training and test sets. As mentioned previously, additional features were added. Feature values were normalised between $[0,12]$. Different scale intervals were tested although their results are not presented. 

The tenique improved average network performance to a mean of 95.11\%($\pm 0.01\%$) (fig. \ref{fig:normalised}). By normalising the data, the network could be trained with larger batch sizes and learning rates without number overflows. However, increasing these values did not further improve performance. 

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{normalised.png}
\caption{Adjusting the learning rate}
\label{fig:normalised}
\end{figure}

\subsection{Two Layer Experiments}

To find what data sets the network does not perform well on, after successfully training a network, the test records the network fails on were viewed.  Most of the failing digits were rotated images. Pixel covariance was used to signal to the network a digit rotation as the covariance would point to the orientation of the digit. This approach unfortunately was not sufficient. Either the covariance did not emerge as a rotation through training, or the network did not have enough potential to generalise. 

\subsubsection{Hidden Unit Count}

An attempt to solve rotation problems was to simply add a hidden layer to the network. The network could now also be optimised in number of hidden units. Other parameters of the network, such as the batch size and epoch count needed readjusting. The below graph (fig. \ref{fig:hiddenunits}) shows improved overall average performance for a batch size of 30 and epoch count of 5000.  Mean network performance in this case was at 96.2\%($\pm 0.1\%$) for a 90 units hidden layer. 
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{hiddenunits.png}
\caption{Adjusting the number of hidden units}
\label{fig:hiddenunits}
\end{figure}

\subsubsection{Weight Decay}
A simple improvement that can improve network generalisation is by weight decay. Decay is a generalisation technique through which weights are constantly pushed towards a value of 0. This has the effect of keeping weights to similar range of value , preventing overshadowing of smaller weights. In some sense it is similar to a form of normalisation.
Regularisation was implemented by simply by adding or subtracting from each weight a ratio of it's current value. The reatio can be seen as another parameter that could be optimised in the network. Experimentally the best performance achieved through weight decay had a performance of 96.45\%($\pm 0.06\%$) (fig. \ref{fig:regularisation}). The performance improvement did not prove significant but had an impact of variability of training. All other parameters remained with previous values ( i.e. batch size 30, hidden units 90,epoch size 90, variable learning rate).

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{regularisation.png}
\caption{Adjusting decay ratio}
\label{fig:regularisation}
\end{figure}

\subsection{Digit Rotations}
As most prediction failures in the testing set were rotated digits, it was necessary to create additional examples of rotations in order to further improve performance. Adding rotations tripled the training set and required adjusting the batch size, epoch count and number of hidden units. The below figure \ref{fig:rotations} demonstrates the effects of adjusting each such parameters. Even though network performance was higher, the predictability of optimisations became more difficult. 

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{rotations.png}
\caption{Adjusting parameters after rotation with decay 0.0001}
\label{fig:rotations}
\end{figure}

The decay ratio for the performance in fig.\ref{fig:rotations} was set to a value of 0.0001 . After increasing the ratio to 0.001 performance slightly improved (fig. \ref{fig:rotations2}). The mean best performance achieved was 96.83\%($\pm 0.06\%$). The lack of predictability in optimisations may be due to the training set being randomly expanded with rotations. The variability in training data may therefore affect prediction performance. The rotations are not reset after each network training, so even though 10 networks are trained, they are likely to all optimise for a training set that is less compatible to the test set. 

However, through this method the best achieved performance was 97.21\%. The trained weights can be loaded and tested against the training set by running \emph{showdigits.m}. The program also flashes images representing digits that failed recognition. 

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{rotations2.png}
\caption{Adjusting decay 0.001, batch size 20 , hidden 90}
\label{fig:rotations2}
\end{figure}

\section{Conclusions}
Surprisingly, a single layer network was able to achieve a significantly high generalisation with  92.5\% prediction performance. The most important improvement was due to implementing a variable learning rate. Whilst performance increased (by 2\% to a total of 94.5\%), the deviation also decreased from 0.3\% to 0.07\%.

The next most significant improvement was adding a hidden layer to the network. The average performance improved by another 1\% with a small increase in standard deviation (to 0.1\%).

With most changes epoch count and batch size had to be readjusted, especially when adding input normalisation and input rotation. Altough the improvements were not significant, the changes were necessary to break the performance barrier of 97.2\%. Variably rotating training data in order to create additional sets proved to make optimisation more difficult. 

\begin{thebibliography}{9}
\bibitem{ucidigits}
UCI machine learning repository: Optical recognition of handwritten digits data set. (1998, July 1). Retrieved January 10, 2016, from http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits

\end{thebibliography}

\end{document}